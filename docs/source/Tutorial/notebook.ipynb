{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "blessed-heavy",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/CPJKU/partitura_tutorial/blob/main/content/Partitura_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legitimate-immigration",
   "metadata": {},
   "source": [
    "# An Introduction to Symbolic Music Processing with Partitura\n",
    "\n",
    "Partitura is python 3 package for symbolic music processing developed and maintained at OFAI Vienna / CP JKU Linz (and other contributors). It's inteded to give a lightweight musical part representation that makes many score properties easily accessible for a variety of tasks. Furthermore it's a very useful I/O utility to parse computer formats of symbolic music. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nonprofit-communication",
   "metadata": {
    "id": "3tvQmcSB7rrL"
   },
   "source": [
    "## 1. Install and import\n",
    "\n",
    "Partitura is available in github https://github.com/CPJKU/partitura\n",
    "\n",
    "You can install it with `pip install partitura`.\n",
    "\n",
    "However if you are interested in features that still have to be officially released, it's better to install the develop branch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facial-quarterly",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PeabdL1k7YC4",
    "outputId": "fcb7d1be-27a1-4c79-c5d3-8cbfa54cae44",
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Install partitura\n",
    "! pip install partitura\n",
    "    \n",
    "# To be able to access helper modules in the repo for this tutorial\n",
    "# (not necessary if the jupyter notebook is run locally instead of google colab)\n",
    "!git clone https://github.com/CPJKU/partitura_tutorial.git\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import sys, os\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), \"partitura_tutorial\", \"content\"))\n",
    "sys.path.insert(0,'/content/partitura_tutorial/content')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "impressed-principle",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import partitura as pt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "toxic-italian",
   "metadata": {
    "id": "CX8wCxyK7emp"
   },
   "source": [
    "#### Dataset for this tutorial\n",
    "\n",
    "In this tutorial we are going to use the [Vienna 4x22 Corpus](https://repo.mdw.ac.at/projects/IWK/the_vienna_4x22_piano_corpus/index.html) which consists of performances of 4 classical piano pieces, which have been aligned to their corresponding scores.\n",
    "\n",
    "The dataset contains:\n",
    "\n",
    "* Scores in MusicXML format (4 scores)\n",
    "* Performances in MIDI files (88 in total, 22 performances per piece, each by a different pianist)\n",
    "* Score to performance alignments in Match file format (88 in total one file per performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "photographic-profession",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the dataset\n",
    "from load_data import init_dataset\n",
    "DATASET_DIR = init_dataset()\n",
    "MUSICXML_DIR = os.path.join(DATASET_DIR, 'musicxml')\n",
    "MIDI_DIR = os.path.join(DATASET_DIR, 'midi')\n",
    "MATCH_DIR = os.path.join(DATASET_DIR, 'match')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valued-helena",
   "metadata": {},
   "source": [
    "## 2. Loading and Exporting Files\n",
    "\n",
    "One of the main use cases of partitura is to load and export common symbolic music formats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sonic-better",
   "metadata": {},
   "source": [
    "### Supported Formats\n",
    "\n",
    "#### Reading\n",
    "\n",
    "##### Symbolic Scores\n",
    "\n",
    "These methods return a `Part`, a `PartGroup` or a list of `Part` objects.\n",
    "\n",
    "|Format| Method|Notes|\n",
    "|:---|:---|:---|\n",
    "|MusicXML| `partitura.load_musicxml`| |\n",
    "|MIDI| `partitura.load_score_midi`|Pitch spelling, key signature (optional) and other information is inferred with methods in `partitura.musicanalysis`. \n",
    "|MEI| `partitura.load_mei`|\n",
    "|Humdrum Kern| `partitura.load_kern`|\n",
    "|MuseScore|`partitura.load_via_musescore`| Requires [MuseScore](https://musescore.org/en). Loads all formats supported by MuseScore. Support on Windows is still untested.\n",
    "\n",
    "##### Symbolic Performances\n",
    "\n",
    "These methods return a `PerformedPart`.\n",
    "\n",
    "|Format| Method|Notes|\n",
    "|:---|:---|:---|\n",
    "|MIDI|`partitura.load_performance_midi`| Loads MIDI file as a performance, including track, channel and program information. Time signature and tempo information are only used to compute the time of the MIDI messages in seconds. Key signature information is ignored\n",
    "\n",
    "##### Alignments\n",
    "\n",
    "These methods return score-to-performance alignment (discussed below).\n",
    "\n",
    "|Format| Method|Notes|\n",
    "|:---|:---|:---|\n",
    "|Match file| `partitura.load_match`| Returns alignment, a performance as `PerformedPart` and optionally a `Part`. See usage below.\n",
    "|Nakamura et al. corresp file | `partitura.load_nakamuracorresp`|\n",
    "|Nakamura et al. match file| `partitura.load_nakamuramatch`|\n",
    "\n",
    "#### Writing\n",
    "\n",
    "##### Symbolic Scores\n",
    "\n",
    "Support for MEI and Humdrum Kern is coming!\n",
    "\n",
    "|Format| Method|Notes|\n",
    "|:---|:---|:---|\n",
    "|MusicXML| `partitura.save_musicxml`|\n",
    "|MIDI| `partitura.save_score_midi`| Includes Key signature, time signature and tempo information.\n",
    "\n",
    "##### Symbolic Performances\n",
    "|Format| Method|Notes|\n",
    "|:---|:---|:---|\n",
    "|MIDI|`partitura.save_performance_midi`| Does not include key signature or time signature information\n",
    "\n",
    "##### Alignments\n",
    "\n",
    "A companion library for music alignment is in preparation!\n",
    "\n",
    "|Format| Method|Notes|\n",
    "|:---|:---|:---|\n",
    "|Match file| `partitura.save_match`| \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd98b602",
   "metadata": {},
   "source": [
    "## 3. Internal Representations\n",
    "\n",
    "### 3.1 The Part Object\n",
    "\n",
    "The ```part``` object is the central object of partitura. It contains a score.\n",
    "- it is a timeline object\n",
    "- time is measured in divs\n",
    "- its elements are timed objects, i.e. they have a starting time and an ending time\n",
    "- external score files are loaded into a part\n",
    "- parts can be exported into score files\n",
    "- it contains many useful methods related to its properties\n",
    "\n",
    "Here's a visual representation of the ```part``` object representing the first measure of Chopin's Nocturne Op. 9 No. 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5754b952",
   "metadata": {},
   "source": [
    "![Timeline_chopin2.png](https://github.com/CPJKU/partitura_tutorial/raw/main/static/Timeline_chopin2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9179e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_musicxml = pt.EXAMPLE_MUSICXML\n",
    "part = pt.load_musicxml(path_to_musicxml)[0]\n",
    "print(part.pretty())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874a18d5",
   "metadata": {},
   "source": [
    "![score_example.png](https://github.com/CPJKU/partitura_tutorial/raw/main/static/score_example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bae1ed",
   "metadata": {},
   "source": [
    "### Part Notes\n",
    "\n",
    "Each ```part``` object contains a list notes. Notes inherit from the ```TimedObject``` class. Like all ```TimedObjects``` they contain a (possibly coincident) start time and end time, encoded as ```TimePoint``` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "423aac6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "part.notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a929369",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(part.notes[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2287849",
   "metadata": {},
   "source": [
    "You can create notes (without timing information) and then add it to a part by specifying start and end times (in divs!). Use each note object only once! You can remove notes from a part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a8293c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_new_note = pt.score.Note(id='n04', step='A', octave=4, voice=1)\n",
    "part.add(a_new_note, start=3, end=15)\n",
    "# print(part.pretty())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eba2fa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "part.remove(a_new_note)\n",
    "# print(part.pretty())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8649483",
   "metadata": {},
   "source": [
    "### Converting from divs to musical units and back\n",
    "\n",
    "Integer divs are useful for encoding scores but unwieldy for human readers. Partitura offers a variety of ```*unit*_maps``` from the timeline unit \"div\" to musical units such as \"beats\" (in two different readings) or \"quarters\". For the inverse operation the corresponding ```inv_*unit*_map``` exist as well. Quarter to div ratio is a fixed value for a ```part``` object, but units like beats might change with time signature, so these ```maps``` are implemented as ```part``` methods.\n",
    "\n",
    "Let's look at how to get the ending position in beats of the last note in our example ```part```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e95eb0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "part.beat_map(part.notes[0].end.t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2b7c10",
   "metadata": {},
   "source": [
    "Some musical information such as key and time signature is valid for a segment of the score but only encoded in one location. To retrieve the \"currently active\" time or key signature at any score position, ```maps``` are available too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05346a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "part.time_signature_map(part.notes[0].end.t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1d6ae9",
   "metadata": {},
   "source": [
    "### Iterating over arbitrary musical objects in a part\n",
    "\n",
    "The ```part``` class contains a central method ```iter_all``` to iterate over all instances of the ```TimedObject``` class or its subclasses of a part. The ```iter_all``` method returns an iterator and takes five optional parameters: \n",
    "- A ```TimedObject``` subclass whose instances are returned. You can find them all in the partitura/partitura/score.py file. Default is all classes.\n",
    "- A ```include_subclasses``` flag. If true, instances of subclasses are returned too. E.g. ``` part.iter_all(pt.score.TimedObject, include_subclasses=True)``` returns all objects or ```part.iter_all(pt.score.GenericNote, include_subclasses=True)``` returns all notes (grace notes, standard notes)\n",
    "- A start time in divs to specify the search interval (default is beginning of the part)\n",
    "- An end time in divs to specify the search interval (default is end of the part)\n",
    "- A ```mode``` parameter to define whether to search for starting or ending objects, defaults to starting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74943a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "for measure in part.iter_all(pt.score.Measure):\n",
    "    print(measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6cbfd044",
   "metadata": {},
   "outputs": [],
   "source": [
    "for note in part.iter_all(pt.score.GenericNote, include_subclasses=True, start=0, end=24):\n",
    "    print(note)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1455a5f",
   "metadata": {},
   "source": [
    "### Example: Adding a new measure and a note at its downbeat\n",
    "\n",
    "Let's use class retrieval, time mapping, and object creation together and add a new measure with a single beat-length note at its downbeat. This code works even if you know nothing about the underlying score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe430921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure out the last measure position, time signature and beat length in divs\n",
    "measures = [m for m in part.iter_all(pt.score.Measure)]\n",
    "last_measure_number = measures[-1].number\n",
    "append_measure_start =  measures[-1].end.t \n",
    "Last_measure_ts = part.time_signature_map(append_measure_start)\n",
    "\n",
    "Last_measure_ts = part.time_signature_map(append_measure_start)\n",
    "one_beat_in_divs_at_the_end = append_measure_start - part.inv_beat_map(part.beat_map(append_measure_start)-1)\n",
    "append_measure_end = append_measure_start +  one_beat_in_divs_at_the_end*Last_measure_ts[0]\n",
    "\n",
    "# add a measure\n",
    "a_new_measure = pt.score.Measure(number = last_measure_number+1)\n",
    "part.add(a_new_measure, start=append_measure_start, end=append_measure_end)\n",
    "# add a note\n",
    "a_new_note = pt.score.Note(id='n04', step='A', octave=4, voice=1)\n",
    "part.add(a_new_note, start=append_measure_start, end=append_measure_start+one_beat_in_divs_at_the_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9d738a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(part.pretty())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129e4c8b",
   "metadata": {},
   "source": [
    "### 3.2 The PerformedPart Object\n",
    "\n",
    "The ```PerformedPart``` class is a wrapper for MIDI files. Its structure is much simpler:\n",
    "- a notes property that consists of list of MIDI notes as dictionaries\n",
    "- a controls property that consists of list of MIDI CC messages\n",
    "- some more utility methods and properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d82a340",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_midifile = pt.EXAMPLE_MIDI\n",
    "performedpart = pt.load_performance_midi(path_to_midifile)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e3090d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "performedpart.notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf8c482",
   "metadata": {},
   "source": [
    "### 3.3 Tiny example with cats on keyboards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6eb12f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def addnote(midipitch, part, voice, start, end, idx):\n",
    "    \"\"\"\n",
    "    adds a single note by midipitch to a part\n",
    "    \"\"\"\n",
    "    offset = midipitch%12\n",
    "    octave = int(midipitch-offset)/12\n",
    "    name = [(\"C\",0),\n",
    "            (\"C\",1),\n",
    "            (\"D\",0),\n",
    "            (\"D\",1),\n",
    "            (\"E\",0),\n",
    "            (\"F\",0),\n",
    "            (\"F\",1),\n",
    "            (\"G\",0),\n",
    "            (\"G\",1),\n",
    "            (\"A\",0),\n",
    "            (\"A\",1),\n",
    "            (\"B\",0)]\n",
    "    # print( id, start, end, offset)\n",
    "    step, alter = name[int(offset)]\n",
    "    part.add(pt.score.Note(id='n{}'.format(idx), step=step, \n",
    "                        octave=int(octave), alter=alter, voice=voice, staff=str((voice-1)%2+1)), \n",
    "                        start=start, end=end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "572e856c",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = 200\n",
    "p = pt.score.Part('CoK', 'Cat on Keyboard', quarter_duration=8)\n",
    "dur = np.random.randint(1,20, size=(4,l+1))\n",
    "ons = np.cumsum(dur, axis = 1)\n",
    "pitch = np.row_stack((np.random.randint(20,40, size=(1,l+1)),\n",
    "                      np.random.randint(60,80, size=(1,l+1)),\n",
    "                      np.random.randint(40,60, size=(1,l+1)),\n",
    "                      np.random.randint(40,60, size=(1,l+1))\n",
    "                     ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9f03a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(l):\n",
    "    for j in range(4):\n",
    "        addnote(pitch[j,k], p, j+1, ons[j,k], ons[j,k]+dur[j,k+1], \"v\"+str(j)+\"n\"+str(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09fb6b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.add(pt.score.TimeSignature(4, 4), start=0)\n",
    "p.add(pt.score.Clef(1, \"G\", line = 3, octave_change=0),start=0)\n",
    "p.add(pt.score.Clef(2, \"G\", line = 3, octave_change=0),start=0)\n",
    "pt.score.add_measures(p)\n",
    "pt.score.tie_notes(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "834582d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pt.save_score_midi(p, \"CatPerformance.mid\", part_voice_assign_mode=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "006f02ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pt.save_musicxml(p, \"CatScore.xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identical-gathering",
   "metadata": {},
   "source": [
    "## 4. Extracting Information from Scores and Performances\n",
    "\n",
    "For many MIR tasks we need to extract specific information out of scores or performances. \n",
    "Two of the most common representations are **note arrays** and **piano rolls**. \n",
    "\n",
    "**Note that there is some overlap in the way that these terms are used.**\n",
    "\n",
    "Partitura provides convenience methods to extract these common features in a few lines!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordinary-psychology",
   "metadata": {},
   "source": [
    "### 4.1 Note Arrays\n",
    "\n",
    "A **note array** is a 2D array in which each row represents a note in the score/performance and each column represents different attributes of the note.\n",
    "\n",
    "In partitura, note arrays are [structured numpy arrays](https://numpy.org/devdocs/user/basics.rec.html), which are ndarrays in which each \"column\" has a name, and can be of different datatypes. \n",
    "This allows us to hold information that can be represented as integers (MIDI pitch/velocity), floating point numbers (e.g., onset time) or strings (e.g., note ids). \n",
    "\n",
    "In this tutorial we are going to cover 3 main cases\n",
    "\n",
    "* Getting a note array from `Part` and `PerformedPart` objects\n",
    "* Extra information and alternative ways to generate a note array\n",
    "* Creating a custom note array from scratch from a `Part` object\n",
    "\n",
    "\n",
    "#### 4.1.1. Getting a note array from `Part` and `PerformedPart` objects\n",
    "\n",
    "##### Getting a note array from `Part` objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "first-basin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note array from a score\n",
    "\n",
    "# Path to the MusicXML file\n",
    "score_fn = os.path.join(MUSICXML_DIR, 'Chopin_op38.musicxml')\n",
    "\n",
    "# Load the score into a `Part` object\n",
    "score_part = pt.load_musicxml(score_fn)\n",
    "\n",
    "# Get note array.\n",
    "score_note_array = score_part.note_array()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "looking-whole",
   "metadata": {},
   "source": [
    "It is that easy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "alternate-coordinate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see the first notes in this note array\n",
    "print(score_note_array[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "toxic-publicity",
   "metadata": {},
   "source": [
    "![example_note_array-2.png](https://raw.githubusercontent.com/CPJKU/partitura_tutorial/main/static/example_note_array.png)\n",
    "\n",
    "By default, Partitura includes some of the most common note-level information in the note array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "subtle-millennium",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(score_note_array.dtype.names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exact-practice",
   "metadata": {},
   "source": [
    "* `onset_beat` is the onset time in beats (as indicated by the time signature). In partitura, negative onset times in beats represent pickup measures. Onset time 0 is the start of the first measure.\n",
    "* `duration_beat` is the duration of the note in beats\n",
    "* `onset_quarter` is the onset time of the note in quarters (independent of the time signature). Similarly to onset time in beats, negative onset times in quarters represent pickup measures and onset time 0 is the start of the first measure.\n",
    "* `duration_quarter`is the duration of the note in quarters\n",
    "* `onset_div` is the onset of the note in *divs*, which is generally a number that allows to represent the note position and duration losslessly with integers. In contrast to onset time in beats or quarters, onset time in divs always start at 0 at the first \"element\" in the score (which might not necessarily be a note).\n",
    "* `duration_div` is the duration of the note in divs.\n",
    "* `pitch` is the MIDI pitch (MIDI note number) of the note\n",
    "* `voice` is the voice of the note (in polyphonic music, where there can be multiple notes at the same time)\n",
    "* `id` is the note id (as appears in MusicXML or MEI formats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compressed-baseball",
   "metadata": {},
   "source": [
    "##### Getting a note array from a  `PerformedPart`\n",
    "\n",
    "In a similar way, we can obtain a note array from a MIDI file in a few lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "passing-lending",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note array from a performance\n",
    "\n",
    "# Path to the MIDI file\n",
    "performance_fn = os.path.join(MIDI_DIR, 'Chopin_op38_p01.mid')\n",
    "\n",
    "# Loading the file to a PerformedPart\n",
    "performance_part = pt.load_performance_midi(performance_fn)\n",
    "\n",
    "# Get note array!\n",
    "performance_note_array = performance_part.note_array()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bright-equity",
   "metadata": {},
   "source": [
    "Since performances contain have other information not included in scores, the default fields in the note array are a little bit different:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "pointed-stupid",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(performance_note_array.dtype.names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cathedral-generator",
   "metadata": {},
   "source": [
    "* `onset_sec` is the onset time of the note in seconds. Onset time in seconds is always $\\geq 0$ (otherwise, the performance would violate the laws of physics ;)\n",
    "* `duration_sec` is the duration of the note in seconds\n",
    "* `pitch` is the MIDI pitch\n",
    "* `velocity` is the MIDI velocity\n",
    "* `track` is the track number in the MIDI file\n",
    "* `channel` is the channel in the MIDI file\n",
    "* `id` is the ID of the notes (automatically generated for MIDI file according to onset time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "subject-reducing",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(performance_note_array[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naval-prescription",
   "metadata": {},
   "source": [
    "We can also create a `PerformedPart` directly from a note array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "spread-performer",
   "metadata": {},
   "outputs": [],
   "source": [
    "note_array = np.array(\n",
    "    [(60, 0, 2, 40),\n",
    "     (65, 0, 1, 15),\n",
    "     (67, 0, 1, 72),\n",
    "     (69, 1, 1, 90),\n",
    "     (66, 2, 1, 80)],\n",
    "    dtype=[(\"pitch\", \"i4\"),\n",
    "           (\"onset_sec\", \"f4\"),\n",
    "           (\"duration_sec\", \"f4\"),\n",
    "           (\"velocity\", \"i4\"),\n",
    "          ]\n",
    ")\n",
    "\n",
    "# Note array to `PerformedPart`\n",
    "performed_part = pt.performance.PerformedPart.from_note_array(note_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "catholic-dealer",
   "metadata": {},
   "source": [
    "We can then export the `PerformedPart` to a MIDI file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "changed-check",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export as MIDI file\n",
    "pt.save_performance_midi(performed_part, \"example.mid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "typical-taxation",
   "metadata": {},
   "source": [
    "#### 4.1.2. Extra information and alternative ways to generate a note array\n",
    "\n",
    "Sometimes we require more information in a note array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "figured-coordinator",
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_score_note_array = pt.utils.music.ensure_notearray(\n",
    "    score_part,\n",
    "    include_pitch_spelling=True, # adds 3 fields: step, alter, octave \n",
    "    include_key_signature=True, # adds 2 fields: ks_fifths, ks_mode\n",
    "    include_time_signature=True, # adds 2 fields: ts_beats, ts_beat_type \n",
    "    # include_metrical_position=True, # adds 3 fields: is_downbeat, rel_onset_div, tot_measure_div\n",
    "    include_grace_notes=True # adds 2 fields: is_grace, grace_type\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "vietnamese-pathology",
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_score_note_array.dtype.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "crude-courage",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(extended_score_note_array[['id', \n",
    "                                 'step', \n",
    "                                 'alter', \n",
    "                                 'octave', \n",
    "                                 'ks_fifths', \n",
    "                                 'ks_mode', #'is_downbeat'\n",
    "                                 ]][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greek-failure",
   "metadata": {},
   "source": [
    "[//]:![example_extended_note_array_cof.png](https://raw.githubusercontent.com/CPJKU/partitura_tutorial/main/static/example_extended_note_array_cof.png)\n",
    "<div>\n",
    "<img src=\"https://raw.githubusercontent.com/CPJKU/partitura_tutorial/main/static/example_extended_note_array_cof.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "global-elite",
   "metadata": {},
   "source": [
    "#### 4.1.3. Creating a custom note array from scratch from a `Part` object\n",
    "\n",
    "Sometimes we are interested in other note-level information that is not included in the standard note arrays. \n",
    "With partitura we can create such a note array easily!\n",
    "\n",
    "For example, imagine that we want a note array that includes whether the notes have an accent mark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "invalid-rhythm",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Path to the MusicXML file\n",
    "score_fn = os.path.join(MUSICXML_DIR, 'Chopin_op10_no3.musicxml')\n",
    "\n",
    "# Load the score into a `Part` object\n",
    "score_part = pt.load_musicxml(score_fn)[0]\n",
    "\n",
    "def get_accent_note_array(part):\n",
    "    \n",
    "    fields = [(\"onset_beat\", \"f4\"), \n",
    "              (\"pitch\", \"i4\"),\n",
    "              (\"accent\", \"i4\")]\n",
    "    # Get all notes in the part\n",
    "    notes = part.notes_tied\n",
    "    # Beat map (maps divs to score time in beats)\n",
    "    beat_map = part.beat_map\n",
    "    N = len(notes)\n",
    "    note_array = np.zeros(N, dtype=fields)\n",
    "    for i, n in enumerate(notes):\n",
    "        # MIDI pitch\n",
    "        note_array[i]['pitch'] = n.midi_pitch\n",
    "        # Get the onset time in beats\n",
    "        note_array[i]['onset_beat'] = beat_map(n.start.t)\n",
    "        \n",
    "        # Iterate over articulations in the note\n",
    "        if n.articulations:\n",
    "            for art in n.articulations:\n",
    "                if art == 'accent':\n",
    "                    note_array[i]['accent'] = 1\n",
    "    return note_array\n",
    "\n",
    "accent_note_array = get_accent_note_array(score_part)\n",
    "\n",
    "accented_note_idxs = np.where(accent_note_array['accent'])\n",
    "print(accent_note_array[accented_note_idxs][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "after-season",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"https://github.com/CPJKU/partitura_tutorial/raw/main/static/example_note_array_accents.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjusted-fundamental",
   "metadata": {},
   "source": [
    "### 4.2 Piano rolls\n",
    "\n",
    "Piano rolls are 2D matrices that represent pitch and time information. The time represents time steps (at a given resolution), while the pitch axis represents which notes are active at a given time step. We can think of piano rolls as the symbolic equivalent of spectrograms. \n",
    "\n",
    "#### Extracting a piano roll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "essential-academy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: change the example\n",
    "# Path to the MusicXML file\n",
    "score_fn = os.path.join(MUSICXML_DIR, 'Chopin_op10_no3.musicxml')\n",
    "\n",
    "# Load the score\n",
    "score_part = pt.load_musicxml(score_fn)\n",
    "# compute piano roll\n",
    "pianoroll = pt.utils.compute_pianoroll(score_part)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entire-nitrogen",
   "metadata": {},
   "source": [
    "The `compute_pianoroll` method has a few arguments to customize the resulting piano roll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "massive-monaco",
   "metadata": {},
   "outputs": [],
   "source": [
    "piano_range = True\n",
    "time_unit = 'beat'\n",
    "time_div = 10\n",
    "pianoroll = pt.utils.compute_pianoroll(\n",
    "    note_info=score_part, # a `Part`, `PerformedPart` or a note array\n",
    "    time_unit=time_unit, # beat, quarter, div, sec, etc. (depending on note_info)\n",
    "    time_div=time_div, # Number of cells per time unit\n",
    "    piano_range=piano_range # Use range of the piano (88 keys)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quality-coast",
   "metadata": {},
   "source": [
    "An important thing to remember is that in piano rolls generated by `compute_pianoroll`, rows (the vertical axis) represent the pitch dimension and the columns (horizontal) the time dimension. \n",
    "This results in a more intuitive way of plotting the piano roll. \n",
    "For other applications the transposed version of this piano roll might be more useful (i.e., rows representing time steps and columns representing pitch information).\n",
    "\n",
    "Since piano rolls can result in very large matrices where most of the elements are 0, the output of `compute_pianoroll` is a [scipy sparse matrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csc_matrix.html). To convert it to a regular numpy array, we can simply use `pianoroll.toarray()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intended-answer",
   "metadata": {},
   "source": [
    "Let's plot the piano roll!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "mature-dylan",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(20, 10))\n",
    "ax.imshow(pianoroll.toarray(), origin=\"lower\", cmap='gray', interpolation='nearest', aspect='auto')\n",
    "ax.set_xlabel(f'Time ({time_unit}s/{time_div})')\n",
    "ax.set_ylabel('Piano key' if piano_range else 'MIDI pitch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funky-tract",
   "metadata": {},
   "source": [
    "In some cases, we want to know the \"coordinates\" of each of the notes in the piano roll. The `compute_pianoroll` method includes an option to return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "palestinian-owner",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pianoroll, note_indices = pt.utils.compute_pianoroll(score_part, return_idxs=True)\n",
    "\n",
    "# MIDI pitch, start, end\n",
    "print(note_indices[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "economic-denial",
   "metadata": {},
   "source": [
    "#### Generating a note array from a piano roll\n",
    "\n",
    "Partitura also includes a method to generate a note array from a piano roll, which can be used to generate a MIDI file. \n",
    "This method would be useful, e.g., for music generation tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "parental-links",
   "metadata": {},
   "outputs": [],
   "source": [
    "pianoroll = pt.utils.compute_pianoroll(score_part)\n",
    "\n",
    "new_note_array = pt.utils.pianoroll_to_notearray(pianoroll)\n",
    "\n",
    "# We can export the note array to a MIDI file\n",
    "ppart = pt.performance.PerformedPart.from_note_array(new_note_array)\n",
    "\n",
    "pt.save_performance_midi(ppart, \"newmidi.mid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floating-madison",
   "metadata": {},
   "source": [
    "## 5. Handling Alignment Information (Match files)\n",
    "\n",
    "### 5.1 Loading Alignments\n",
    "An important use case of partitura is to handle symbolic alignment information\n",
    "\n",
    "**Note that partitura itself does not contain methods for alignment**\n",
    "\n",
    "Partitura supports 2 formats for encoding score-to-performance alignments\n",
    "\n",
    "* Our match file format, introduced by Gerhard et al. ;)\n",
    "    * Datasets including match files: Vienna4x22, Magaloff, Zeilinger, Batik, and soon ASAP!\n",
    "* The format introduced by [Nakamura et al. (2017)](https://eita-nakamura.github.io/articles/EN_etal_ErrorDetectionAndRealignment_ISMIR2017.pdf)\n",
    "\n",
    "Let's load an alignment!\n",
    "\n",
    "We have two common use cases\n",
    "\n",
    "* We have both the match file and the symbolic score file (e.g., MusicXML or MEI)\n",
    "* We have only the match file (only works for our format!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crucial-virus",
   "metadata": {},
   "source": [
    "#### 5.1.1. Loading an alignment if we only have a match file\n",
    "\n",
    "A useful property of match files is that they include information about the **score and the performance**. Therefore, it is possible to create both a  `Part` and a `PerformedPart` directly from a match file.\n",
    "\n",
    "* Match files contain all information included in performances in MIDI files, i.e., a MIDI file could be reconstructed from a match file.\n",
    "\n",
    "* Match files include all information information about pitch spelling and score position and duration of the notes in the score, as well as time and key signature information, and can encode some note-level markings, like accents. Nevertheless, it is important to note that the score information included in a match file is not necessarily complete. For example, match files do not generally include dynamics or tempo markings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "rolled-cloud",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the match\n",
    "match_fn = os.path.join(MATCH_DIR, 'Chopin_op10_no3_p01.match')\n",
    "# loading a match file\n",
    "performed_part, alignment, score_part = pt.load_match(match_fn, create_part=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wooden-looking",
   "metadata": {},
   "source": [
    "#### 5.1.2. Loading an alignment if we have both score and match files\n",
    "\n",
    "In many cases, however, we have access to both the score and match files. Using the original score file has a few advantages:\n",
    "\n",
    "* It ensures that the score information is correct. Generating a `Part` from a match file involves inferring information for non-note elements (e.g., start and end time of the measures, voice information, clefs, staves, etc.).\n",
    "* If we want to load several performances of the same piece, we can load the score only once!\n",
    "\n",
    "This should be the preferred way to get alignment information!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "latest-smell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the match\n",
    "match_fn = os.path.join(MATCH_DIR, 'Chopin_op10_no3_p01.match')\n",
    "# Path to the MusicXML file\n",
    "score_fn = os.path.join(MUSICXML_DIR, 'Chopin_op10_no3.musicxml')\n",
    "# Load the score into a `Part` object\n",
    "score_part = pt.load_musicxml(score_fn)[0]\n",
    "\n",
    "# loading a match file\n",
    "performed_part, alignment = pt.load_match(match_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pending-college",
   "metadata": {},
   "source": [
    "Score-to-performance alignments are represented by lists of dictionaries, which contain the following keys:\n",
    "\n",
    "* `label`\n",
    "\n",
    "    * `'match'`: there is a performed note corresponding to a score note\n",
    "    * `'insertion'`: the performed note does not correspond to any note in the score\n",
    "    * `'deletion'`: there is no performed note corresponding to a note in the score\n",
    "    * `'ornament'`: the performed note corresponds to the performance of an ornament (e.g., a trill). These notes are matched to the main note in the score. Not all alignments (in the datasets that we have) include ornamnets! Otherwise, ornaments are just treated as insertions.\n",
    "* `score_id`: id of the note in the score (in the `Part` object) (only relevant for matches, deletions and ornaments)\n",
    "* `performance_id`: Id of the note in the performance (in the `PerformedPart`) (only relevant for matches, insertions and ornaments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "radio-interim",
   "metadata": {},
   "outputs": [],
   "source": [
    "alignment[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exact-decrease",
   "metadata": {},
   "source": [
    "### 5.2 Getting information from the alignments\n",
    "\n",
    "Partitura includes a few methods for getting information from the alignments.\n",
    "\n",
    "Let's start by getting the subset of score notes that have a corresponding performed note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "published-understanding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note array of the score\n",
    "snote_array = score_part.note_array()\n",
    "# note array of the performance\n",
    "pnote_array = performed_part.note_array()\n",
    "# indices of the notes that have been matched\n",
    "matched_note_idxs = pt.utils.music.get_matched_notes(snote_array, pnote_array, alignment)\n",
    "\n",
    "# note array of the matched score notes\n",
    "matched_snote_array = snote_array[matched_note_idxs[:, 0]]\n",
    "# note array of the matched performed notes\n",
    "matched_pnote_array = pnote_array[matched_note_idxs[:, 1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alike-doctor",
   "metadata": {},
   "source": [
    "#### Comparing tempo curves\n",
    "\n",
    "In this example, we are going to compare tempo curves of different performances of the same piece. Partitura includes a utility function called `get_time_maps_from_alignment`which creates functions (instances of [`scipy.interpolate.interp1d`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.interp1d.html)) that map score time to performance time (and the other way around)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "offshore-bridal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all match files\n",
    "matchfiles = glob.glob(os.path.join(MATCH_DIR, 'Chopin_op10_no3_p*.match'))\n",
    "matchfiles.sort()\n",
    "\n",
    "# Score time from the first to the last onset\n",
    "score_time = np.linspace(snote_array['onset_beat'].min(),\n",
    "                         snote_array['onset_beat'].max(),\n",
    "                         100)\n",
    "# Include the last offset\n",
    "score_time_ending = np.r_[\n",
    "    score_time, \n",
    "    (snote_array['onset_beat'] + snote_array['duration_beat']).max() # last offset\n",
    "]\n",
    "\n",
    "tempo_curves = np.zeros((len(matchfiles), len(score_time)))\n",
    "for i, matchfile in enumerate(matchfiles):\n",
    "    # load alignment\n",
    "    performance, alignment = pt.load_match(matchfile)\n",
    "    ppart = performance[0]\n",
    "    # Get score time to performance time map\n",
    "    _, stime_to_ptime_map = pt.musicanalysis.performance_codec.get_time_maps_from_alignment(\n",
    "        ppart, score_part, alignment)\n",
    "    # Compute naïve tempo curve\n",
    "    performance_time = stime_to_ptime_map(score_time_ending)\n",
    "    tempo_curves[i,:] = 60 * np.diff(score_time_ending) / np.diff(performance_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "brazilian-honey",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(15, 8))\n",
    "color = plt.cm.rainbow(np.linspace(0, 1, len(tempo_curves)))\n",
    "for i, tempo_curve in enumerate(tempo_curves):\n",
    "    ax.plot(score_time, tempo_curve, \n",
    "            label=f'pianist {i + 1:02d}', alpha=0.4, c=color[i])\n",
    "\n",
    "# plot average performance\n",
    "ax.plot(score_time, tempo_curves.mean(0), label='average', c='black', linewidth=2)\n",
    "\n",
    "# get starting time of each measure in the score\n",
    "measure_times = score_part.beat_map([measure.start.t for measure in score_part.iter_all(pt.score.Measure)])\n",
    "# do not include pickup measure\n",
    "measure_times = measure_times[measure_times >= 0]\n",
    "ax.set_title('Chopin Op. 10 No. 3')\n",
    "ax.set_xlabel('Score time (beats)')\n",
    "ax.set_ylabel('Tempo (bpm)')\n",
    "ax.set_xticks(measure_times)\n",
    "plt.legend(frameon=False, bbox_to_anchor = (1.15, .9))\n",
    "plt.grid(axis='x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2372b392",
   "metadata": {},
   "source": [
    "## The end of the tutorial, the start of your yet untold adventures in symbolic music processing...\n",
    "\n",
    "Thank you for trying out partitura! We hope it serves you well. \n",
    "\n",
    "If you miss a particular functionality or encounter a bug, we appreciate it if you raise an issue on GitHub: https://github.com/CPJKU/partitura/issues"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNCzhR7KnjsrjKGf/HDyInO",
   "include_colab_link": true,
   "name": "Partitura tutorial",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
